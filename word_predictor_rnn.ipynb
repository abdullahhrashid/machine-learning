{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#trying to implement an rnn for next word prediction but im failing miserably at it"
      ],
      "metadata": {
        "id": "j_C1bgQwmiap"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W7LlGRXkQtGv"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w2R2v74nREIG"
      },
      "outputs": [],
      "source": [
        "#some functions ill be using for data preprocessing\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]','',text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "vocab = {'<UNK>':0}\n",
        "\n",
        "def generate_vocab(texts):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        counter.update(tokenize(text))\n",
        "    for word in counter:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "def encode_text(text):\n",
        "    return [vocab.get(token,vocab['<UNK>']) for token in tokenize(str(text))]\n",
        "\n",
        "def build_dataset(dataset):\n",
        "    input_set = []\n",
        "    output_set = []\n",
        "\n",
        "    for sentence in dataset:\n",
        "        for i in range(0,len(sentence)-1):\n",
        "            input_set.append(sentence[:i+1])\n",
        "            output_set.append(sentence[i+1])\n",
        "\n",
        "    return zip(input_set,output_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A5sW-tGdv3UT"
      },
      "outputs": [],
      "source": [
        "#loading data\n",
        "df = pd.read_csv('dataset_book.txt', encoding='latin1', sep='\\t', quotechar='\"', names=['text'])\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "generate_vocab(df.text)\n",
        "df['text'] = df['text'].apply(encode_text)\n",
        "\n",
        "train_set, test_set = train_test_split(df.text,train_size=0.8,test_size=0.2,shuffle=False)\n",
        "\n",
        "train_set = train_set.reset_index(drop=True)\n",
        "test_set = test_set.reset_index(drop=True)\n",
        "\n",
        "train_set = build_dataset(list(train_set))\n",
        "train_set = pd.DataFrame(train_set,columns=['words','prediction'])\n",
        "\n",
        "test_set = build_dataset(list(test_set))\n",
        "test_set = pd.DataFrame(test_set,columns=['words','prediction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4HfusEAbRONT"
      },
      "outputs": [],
      "source": [
        "#custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df):\n",
        "        self.x = df.words\n",
        "        self.y = df.prediction\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return torch.tensor(self.x.iloc[index], dtype=torch.long), torch.tensor(self.y.iloc[index], dtype=torch.long)\n",
        "\n",
        "def collatefn(batch):\n",
        "    text, labels = zip(*batch)\n",
        "    text = pad_sequence(text,padding_value=0,batch_first=True,padding_side='left')\n",
        "    return text,torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sxBWa8Aoy6GJ"
      },
      "outputs": [],
      "source": [
        "#creating dataloaders\n",
        "train_set = CustomDataset(train_set)\n",
        "train_loader = DataLoader(train_set,batch_size=32,shuffle=False,collate_fn=collatefn)\n",
        "\n",
        "test_set = CustomDataset(test_set)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=32, collate_fn=collatefn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s5H-azOOS2iW"
      },
      "outputs": [],
      "source": [
        "#defining the rnn\n",
        "class rnn(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size,120,padding_idx=0)\n",
        "        nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n",
        "        self.rnn1 = nn.LSTM(120,500, batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(500)\n",
        "        self.rnn2 = nn.LSTM(500, 1000,batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(1000)\n",
        "        self.rnn3 = nn.LSTM(1000,2000,batch_first=True)\n",
        "        self.bn3 = nn.BatchNorm1d(2000)\n",
        "        self.rnn4 = nn.LSTM(2000,3000,batch_first=True)\n",
        "        self.bn4 = nn.BatchNorm1d(3000)\n",
        "        self.fc = nn.Linear(3000,vocab_size)\n",
        "\n",
        "    def forward(self,X):\n",
        "         out = self.embed(X)\n",
        "\n",
        "         out, _ = self.rnn1(out)\n",
        "         out = self.bn1(out.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "         out, _ = self.rnn2(out)\n",
        "         out = self.bn2(out.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "         out, _ = self.rnn3(out)\n",
        "         out = self.bn3(out.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "         out, _ = self.rnn4(out)\n",
        "         out = self.bn4(out.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "         out = self.fc(out)\n",
        "         return out\n",
        "\n",
        "#initializing the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = rnn(len(vocab))\n",
        "model = model.to(device)\n",
        "\n",
        "#hyperparameters\n",
        "n_epochs = 60\n",
        "learning_rate = 0.0001\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "jQud4LGxlsyL",
        "outputId": "0dfee6d4-e19a-4518-c749-ac0c60ce63bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1126577370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#training loop\n",
        "graph = {}\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        y_hat = model(batch_x)\n",
        "\n",
        "        y_hat = y_hat[:,-1,:]\n",
        "\n",
        "        l = loss(y_hat,batch_y)\n",
        "\n",
        "        l.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += l.item() * batch_x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    graph[epoch+1] = avg_loss\n",
        "\n",
        "    #if (epoch+1)%10 == 0:\n",
        "    print(f'For Epoch {epoch+1}: Loss = {avg_loss}')\n",
        "\n",
        "#learning curve\n",
        "plt.title('Loss Vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(graph.keys(),graph.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhzJD7tDmuqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db995658-29a2-4fbc-9da8-1d0713d06173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Accuracy: 17.18%\n"
          ]
        }
      ],
      "source": [
        "#model evaluation for training set\n",
        "model.eval()\n",
        "\n",
        "total_correct = 0\n",
        "total = 0\n",
        "\n",
        "for batch_x, batch_y in train_loader:\n",
        "\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    prediction = model(batch_x)\n",
        "\n",
        "    prediction = prediction[:,-1,:]\n",
        "\n",
        "    _ , output = torch.max(prediction, 1)\n",
        "\n",
        "    total_correct += (output==batch_y).sum().item()\n",
        "\n",
        "    total += batch_y.size(0)\n",
        "\n",
        "\n",
        "accuracy = total_correct / total * 100\n",
        "print(f'Training Set Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation for test set\n",
        "model.eval()\n",
        "\n",
        "total_correct = 0\n",
        "total = 0\n",
        "\n",
        "for batch_x, batch_y in test_loader:\n",
        "\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    prediction = model(batch_x)\n",
        "\n",
        "    prediction = prediction[:,-1,:]\n",
        "\n",
        "    _ , output = torch.max(prediction, 1)\n",
        "\n",
        "    total_correct += (output==batch_y).sum().item()\n",
        "\n",
        "    total += batch_y.size(0)\n",
        "\n",
        "\n",
        "accuracy = total_correct / total * 100\n",
        "print(f'Test Set Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8P3VbF_7fXU",
        "outputId": "e802a959-3be6-4f32-8739-0ad8b28c50ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 7.12%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}